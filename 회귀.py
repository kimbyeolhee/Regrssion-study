# -*- coding: utf-8 -*-
"""회귀.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12arlu7eXJ78T-O0tIXp2n5ojwxZmJ0HA

# Linear Regression
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from scipy import stats
from sklearn.datasets import load_boston
# %matplotlib inline

boston = load_boston()

bostonDF= pd.DataFrame(boston.data, columns = boston.feature_names)

bostonDF['PRICE'] = boston.target
print('Boston 데이터셋 크기:', bostonDF.shape)
bostonDF.head()

"""**각 컬럼별로 주택가격에 미치는 영향도를 조사**"""

fig, axs= plt.subplots(figsize=(16,8),ncols=4,nrows=2)
lm_features = ['RM','ZN','INDUS','NOX','AGE','PTRATIO','LSTAT','RAD']
for i , feature in enumerate(lm_features):
  row = int(i/4)
  col = i%4

  sns.regplot(x=feature, y='PRICE', data=bostonDF, ax=axs[row][col])

"""데이터 세트 train, test로 분리하고 학습/예측/평가 수행"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

y_target = bostonDF['PRICE']
X_data = bostonDF.drop(['PRICE'],axis=1,inplace=False)

X_train, X_test, y_train, y_test = train_test_split(X_data,y_target, test_size=0.3, random_state=156)

## Linear Regression OLS
lr = LinearRegression()
lr.fit(X_train,y_train)
y_preds = lr.predict(X_test)
mse = mean_squared_error(y_test, y_preds)
rmse = np.sqrt(mse)

print('MSE : {0: .3f}, RMSE : {1: .3F}'.format(mse,rmse))
print('Variance score : {0:.3f}'.format(r2_score(y_test,y_preds)))

print('절편값:', lr.intercept_)
print('회귀 계수값:',np.round(lr.coef_,1))

## 회귀 계수를 큰 값 순으로 정렬
coeff = pd.Series(data=np.round(lr.coef_,1), index=X_data.columns)
coeff.sort_values(ascending=False)

"""NOX 값이 몹시 크다."""

from sklearn.model_selection import cross_val_score

y_target = bostonDF['PRICE']
X_data = bostonDF.drop(['PRICE'],axis=1,inplace=False)
lr = LinearRegression()

## cross_val_score()로 5 Fold 셋으로 MSE를 구한 뒤 RMSE 계산
neg_mse_scores = cross_val_score(lr,X_data,y_target, scoring="neg_mean_squared_error",cv=5)
rmse_scores =  np.sqrt(-1 * neg_mse_scores) # 다시 반환된 값에 -1을 곱해야 원래 모델에서 계산된 MSE값
avg_rmse = np.mean(rmse_scores)

## cross_val_score(scoring="neg_mse_scores"로 반환된 값은 모두 음수) 
print("5 folds의 개별 Negative MSE scores:", np.round(neg_mse_scores,2))
print("5 folds의 개별 RMSE scores:", np.round(rmse_scores,2))
print("5 folds의 평균 RMSE scores:", np.round(avg_rmse,2))

"""사이킷런의 지표평가 기준은 높은 지표 값일수록 좋은 모델로 판단하는데,
회귀는 MSE 값이 낮을수록 좋은 회귀모델이다.
따라서 metric 평가 기준에 MSE를 부합시키기 위해 계산된 값에 -1을 곱해서 반환

# Polynomial Regression - 다항회귀
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error , r2_score
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
import numpy as np

# dataset load
boston = load_boston()

bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names)

bostonDF['PRICE'] = boston.target
print('Boston 데이터셋 크기:', bostonDF.shape)

y_target = bostonDF['PRICE']
X_data = bostonDF.drop(['PRICE'],axis=1, inplace=False)

X_train, X_test, y_train, y_test = train_test_split(X_data, y_target, test_size=0.3, random_state=156)

## Pipeline
p_model = Pipeline([('poly',PolynomialFeatures(degree=2, include_bias=False)),
                    ('linear',LinearRegression())])
p_model.fit(X_train, y_train)
y_preds = p_model.predict(X_test)
mse = mean_squared_error(y_test, y_preds)
rmse = np.sqrt(mse)

print('MSE : {0:3f}, RMSE : {1: .3f}'.format(mse, rmse))
print('Variance Score : {0: .3f}'.format(r2_score(y_test,y_preds)))

"""## Ridge Regression
* 규제 선형회귀 유형
* w의 제곱에 패널티 부여 - 계수 값의 크기조정
* (Lasso는 w의 절댓값에 패널티 부여 - 피처의 개수를 줄임)
* (Elastic Net은 둘을 혼합)
"""

from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

# dataset load
boston = load_boston()

bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names)

bostonDF['PRICE'] = boston.target
print('Boston 데이터셋 크기:', bostonDF.shape)

y_target = bostonDF['PRICE']
X_data = bostonDF.drop(['PRICE'],axis=1, inplace=False)

ridge = Ridge(alpha = 10)
neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring="neg_mean_squared_error", cv=5)
rmse_scores = np.sqrt(-1 * neg_mse_scores)
avg_rmse = np.mean(rmse_scores)

print("5 folds의 개별 Negative MSE scores:", np.round(neg_mse_scores,2))
print("5 folds의 개별 RMSE scores:", np.round(rmse_scores,2))
print("5 folds의 평균 RMSE scores:", np.round(avg_rmse,2))

# alpha 파라미터의 값을 다르게 설정하면서 RMSE 확인
alphas = [0, 0.1, 1, 10, 100]

for alpha in alphas : 
  ridge = Ridge(alpha = alpha)

  neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring="neg_mean_squared_error", cv=5)
  avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))
  print("alpha : {0} 일 때 5 folds의 평균 RMSE : {1: .3f}" .format(alpha, avg_rmse))

"""시각화"""

fog , axs = plt.subplots(figsize=(18,6), nrows=1,ncols=5)
coeff_df = pd.DataFrame() #alpha에 따른 회귀계수 값 저장

for pos , alpha in enumerate(alphas) :
  ridge = Ridge(alpha= alpha)
  ridge.fit(X_data,y_target) #학습

  #alpha에 따른 회귀 계수를 Series화
  coeff = pd.Series(data=ridge.coef_ , index=X_data.columns)
  colname = 'alpha:'+str(alpha)
  coeff_df[colname] = coeff

  #시각화
  coeff = coeff.sort_values(ascending=False)
  axs[pos].set_title(colname)
  axs[pos].set_xlim(-3,6)
  sns.barplot(x=coeff.values, y=coeff.index, ax=axs[pos])

plt.show()

ridge_alphas =[0,0.1,1,10,100]
sort_column = 'alpha:' + str(ridge_alphas[0])
coeff_df.sort_values(by=sort_column, ascending=False)

"""## Lasso Regression"""

from sklearn.linear_model import Lasso, ElasticNet

def get_linear_reg_eval(model_name, params=None, X_data_n=None, y_target_n=None, verbose=True):
  coeff_df = pd.DataFrame()
  if verbose : print('######', model_name , '######')
  for param in params:
    if model_name =='Ridge' : model = Ridge(alpha=param)
    elif model_name == 'Lasso': model = Lasso(alpha=param)
    elif model_name == 'ElasticNet': model = ElasticNet(alpha=param, l1_ratio=0.7)
    neg_mse_scores = cross_val_score(model, X_data_n, y_target_n, scoring="neg_mean_squared_error", cv=5)

    avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))
    print('alpha {0}일 때 5 fold의 평균 RMSE : {1: .3f}'.format(param,avg_rmse))

    model.fit(X_data,y_target)

    coeff = pd.Series(data=model.coef_ , index=X_data.columns)
    colname = 'alpha'+str(param)
    coeff_df[colname] = coeff
  return coeff_df

lasso_alphas=[0.07,0.1,0.5,1,3]
coeff_lasso_df = get_linear_reg_eval('Lasso', params=lasso_alphas, X_data_n=X_data, y_target_n=y_target)

sort_column= 'alpha'+str(lasso_alphas[0])
coeff_lasso_df.sort_values(by=sort_column,ascending=False)

"""## Elastic net"""

Elastic_alphas=[0.07,0.1,0.5,1,3]
coeff_elastic_df = get_linear_reg_eval('ElasticNet', params=lasso_alphas, X_data_n=X_data, y_target_n=y_target)

sort_column= 'alpha'+str(Elastic_alphas[0])
coeff_elastic_df.sort_values(by=sort_column,ascending=False)



